# grid args
runner: "research.conditional.train.cc_train"
time: "2-00:00:00"
n_gpus: 1

# train params
params:
  # logging
  use_neptune: true
  project_name: "pmtest/llm-efficiency"
  name: "no_ln"
  tags:
    - "tomek_trial_run"

  # model/train config
  "^model_type":
    - "gpt"
  batch_size: 128
  cutoff: 512 # sequence length
  dmodel: 768
  dff: 3072
  n_blocks: 4
  n_steps: 100000
  mixed_precision: true
  logging_interval_heavy: 5000
  ^learning_rate: [0.0001]
  dataset_type: "wiki"

  # feedforward config
  "^ff_mode": ["expert_choice"]
  "^n_experts": [32]
  "^granularity_expert_config":
    - true
  "^total_experts_width":
    - 98304
  "^effective_dff":
    - 3072
  logging_interval_loss: 1000
  every_other_layer: true # MoE on every second layer