# grid args
runner: "research.conditional.train.cc_train"
time: "2-00:00:00"
n_gpus: 1

# train params
params:
  # logging
  use_neptune: true
  project_name: "pmtest/llm-efficiency"
  name: "no_ln"
  tags:
    - "test_ddp_debug"

  # model/train config
  "^model_type":
    - "bert"
  batch_size: 8
  cutoff: 512 # sequence length
  dmodel: 768
  dff: 128
  n_blocks: 4
  n_steps: 100000
  mixed_precision: true
  logging_interval_heavy: 100
  ^learning_rate: [0.0001]
#  dataset_type: "wiki"

  # feedforward config
  "^ff_mode": ["expert_choice"]
  "^n_experts": [32]
  "^granularity_expert_config":
    - true
  "^total_experts_width":
    - 1024
  "^effective_dff":
    - 128
  logging_interval_loss: 10
  every_other_layer: true # MoE on every second layer
#  hack_name: batch_size